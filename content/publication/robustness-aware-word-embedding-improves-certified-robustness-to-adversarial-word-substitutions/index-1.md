---
title: Robustness-Aware Word Embedding Improves Certified Robustness to
  Adversarial Word Substitutions
publication_types:
  - "1"
authors:
  - admin
  - Yichen Yang
  - Di He
  - Kun He
author_notes:
  - Equal contribution
  - Equal contribution
  - ""
  - ""
publication: In *Findings of ACL 2023*
abstract: Natural Language Processing (NLP) models have gained great success on
  clean texts, but they are known to be vulnerable to adversarial examples
  typically crafted by synonym substitutions. In this paper, we target to solve
  this problem and find that word embedding is important to the certified
  robustness of NLP models. Given the findings, we propose the Embedding
  Interval Bound Constraint (EIBC) triplet loss to train robustness-aware word
  embeddings for better certified robustness.
draft: false
featured: true
tags:
  - Adversaral Robustness
  - Certified Robustness
  - Natural Language Processing
image:
  filename: avatar.jpg
  focal_point: Smart
  preview_only: false
summary: >-
  {{% callout note %}}

  Click the *Cite* button above to demo the feature to enable visitors to import publication metadata into their reference management software.

  {{% /callout %}}


  {{% callout note %}}

  Create your slides in Markdown - click the *Slides* button to check out the example.

  {{% /callout %}}


  Supplementary notes can be added here, including [code, math, and images](https://wowchemy.com/docs/writing-markdown-latex/).
date: 2023-05-06T10:31:25.446Z
---
